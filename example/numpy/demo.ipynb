{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of MXNet Numpy Module\n",
    "\n",
    "## Operator Namespaces for Imperative Programming\n",
    "- `mxnet.numpy`: Regular NumPy operators\n",
    "- `mxnet.numpy.random`: NumPy random operators\n",
    "- `mxnet.numpy.linalg`: NumPy linear algebra operators\n",
    "- `mxnet.numpy.ext`: Operators implemented in MXNet that do not exist in official NumPy\n",
    "\n",
    "## Operator Namespaces for Gluon\n",
    "`F` can be either `mxnet.ndarray` or `mxnet.symbol`.\n",
    "- `F.np`: Regular NumPy operators\n",
    "- `F.np.random`: NumPy random operators\n",
    "- `F.np.linalg`: NumPy linear algebra operators\n",
    "- `F.np.ext`: Operators implemented in MXNet that do not exist in official NumPy\n",
    "\n",
    "## New `ndarray` and `symbol`\n",
    "`mxnet.numpy.ndarray` and `mxnet.symbol.numpy._NumpySymbol` (not visible to users)\n",
    "- Same name as in the official NumPy package\n",
    "- Dispatch convience fluent method calls to MXNet Numpy operators\n",
    "- Override many convenience fluent methods that do not exist in the official NumPy ndarray\n",
    "- Make the behavior of built-in methods consistent with the official NumPy\n",
    "    - Indexing: `__getitem__` and `__setitem__`\n",
    "    - Many binary element-wise with broadcasting, not supported in `mxnet.symbol.Symbol`\n",
    "    \n",
    "## Examples of ndarray and symbol Basics\n",
    "### Scalar and zero-size tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import numpy as np\n",
    "\n",
    "# use numpy-compatible semantics\n",
    "mx.set_np_compat(True)\n",
    "\n",
    "# create a scalar tensor\n",
    "x = np.array(3.14)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = x.item()  # copy the element from the scalar tensor to a python scalar\n",
    "print('s = {}'.format(str(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scalar tensors with only one element 1.0\n",
    "y = np.ones(())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a zero-size tensor\n",
    "x = np.ones((5, 4, 0, 6))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose the zero-size tensor\n",
    "y = np.transpose(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion between classic and numpy ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a classic MXNet NDArray\n",
    "x = mx.nd.random.uniform(shape=(2, 3))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert classic NDArray type to mxnet.numpy.ndarray with zero-copy\n",
    "y = x.as_np_ndarray()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing y's content changes x's content too\n",
    "y[:] = 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mxnet.numpy.ndarray to classic NDArray with zero-copy\n",
    "z = y.as_classic_ndarray()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing z's content changes y's content too\n",
    "z[:] = 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary element-wise operations with broadcasting in new and old symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "class TestBinaryBroadcast(gluon.HybridBlock):\n",
    "    def hybrid_forward(self, F, x1, x2):\n",
    "        print(\"x1 type:\", str(type(x1)))\n",
    "        print(\"x2 type:\", str(type(x2)))\n",
    "        return x1 + x2\n",
    "\n",
    "net = TestBinaryBroadcast()\n",
    "x1 = mx.nd.ones((2, 1))\n",
    "x2 = mx.nd.ones((1, 3))\n",
    "out = net(x1, x2)  # ok: imperative execution supports broadcasting\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hybridize()  # mark the block for execution using a computational graph\n",
    "try:\n",
    "    out = net(x1, x2)  # error: old symbol `+` operation does not support broadcasting\n",
    "    assert False  # should not reach here\n",
    "except mx.MXNetError:\n",
    "    print(\"ERROR: cannot perform broadcast add for two symbols of mxnet.sym.Symbol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBinaryBroadcast2(gluon.HybridBlock):\n",
    "    def hybrid_forward(self, F, x1, x2):\n",
    "        print(\"x1 type:\", str(type(x1)))\n",
    "        print(\"x2 type:\", str(type(x2)))\n",
    "        return x1.as_np_ndarray() + x2  # convert x1 to new numpy ndarray/symbol\n",
    "\n",
    "net2 = TestBinaryBroadcast2()\n",
    "net2.hybridize()\n",
    "\n",
    "out =net2(x1, x2)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TestBinaryBroadcast()  # Create a new block object to clear the graph\n",
    "net.hybridize()  # mark the block for execution using a computational graph\n",
    "\n",
    "x1 = x1.as_np_ndarray()  # convert x1 to np.ndarray so that _NumpySymbol will be used in graph construction\n",
    "x2 = x2.as_np_ndarray()  # convert x2 to np.ndarray so that _NumpySymbol will be used in graph construction\n",
    "out = net(x1, x2)  # ok: `+` operation supports broadcasting for _NumpySymbol\n",
    "print(out)  # mxnet.numpy.ndarray type, because it's from a np operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Linear Regression Model\n",
    "Let's consider a simple linear regression model as the following.\n",
    "Given dataset `{x, y}`, where `x`s represent input examples and `y`s represent observed data, find the parameters `w1` and `w2` for the following model.\n",
    "```\n",
    "y_pred = np.dot(np.maximum(np.dot(x, w1), 0), w2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXNet Numpy Operators in Imperative Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import numpy as np\n",
    "from mxnet import autograd\n",
    "try:\n",
    "    from mxboard import SummaryWriter\n",
    "except ImportError:\n",
    "    SummaryWriter = None\n",
    "\n",
    "# create a summary writer for visualization\n",
    "sw = SummaryWriter(logdir='./logs', flush_secs=2) if SummaryWriter is not None else None\n",
    "\n",
    "# Use numpy-compatible semantics to support scalar tensors\n",
    "mx.set_np_compat(True)\n",
    "\n",
    "# N is number of examples; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = mx.nd.random.normal(shape=(N, D_in)).as_np_ndarray()  # x is of type mxnet.numpy.ndarray\n",
    "y = mx.nd.random.normal(shape=(N, D_out)).as_np_ndarray()  # y is of type mxnet.numpy.ndarray\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = mx.nd.random.normal(shape=(D_in, H)).as_np_ndarray()  # w1 is of type mxnet.numpy.ndarray\n",
    "w1.attach_grad()  # w1.grad is of type mxnet.numpy.ndarray\n",
    "w2 = mx.nd.random.normal(shape=(H, D_out)).as_np_ndarray()  # w2 is of type mxnet.numpy.ndarray\n",
    "w2.attach_grad()  # w2.grad is of type mxnet.numpy.ndarray\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "\n",
    "for t in range(1000):\n",
    "    with autograd.record():\n",
    "        # Forward pass: compute predicted y\n",
    "        h = x.dot(w1)  # equivalent to np.dot(x, w1)\n",
    "        h_relu = np.ext.relu(h)  # equivalent to mx.nd.relu(h)\n",
    "        y_pred = h_relu.dot(w2)  # equivalent to np.dot(h_relu, w2)\n",
    "\n",
    "        # Compute loss\n",
    "        # (y_pred - y) ** 2 calls np.ndarray.__pow__\n",
    "        # sum() calls np.sum() which should return a scalar tensor\n",
    "        loss = ((y_pred - y) ** 2).sum()\n",
    "    # Note that the print function will invoke loss.asnumpy()\n",
    "    print(t, loss)  # loss is a scalar tensor of type mxnet.numpy.ndarray\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    if sw is not None:\n",
    "        sw.add_scalar('loss', loss.item(), global_step=t)  # loss.item() copies the tensor element to a python scalar\n",
    "        if t % 50 == 0:\n",
    "            sw.add_histogram(tag='w1', values=w1, global_step=t)\n",
    "            sw.add_histogram(tag='w2', values=w2, global_step=t)\n",
    "\n",
    "if sw is not None:\n",
    "    sw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXNet Numpy Operators in Gluon `HybridBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "try:\n",
    "    from mxboard import SummaryWriter\n",
    "except ImportError:\n",
    "    SummaryWriter = None\n",
    "\n",
    "# create a summary writer for visualization\n",
    "sw = SummaryWriter(logdir='./logs', flush_secs=2) if SummaryWriter is not None else None\n",
    "\n",
    "# Use numpy-compatible semantics to support scalar tensors\n",
    "mx.set_np_compat(True)\n",
    "\n",
    "\n",
    "class LinearRegression(gluon.HybridBlock):\n",
    "    def __init__(self, num_input_dim=1000, num_hidden_dim=100, num_output_dim=10):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.w1 = self.params.get('w1', shape=(num_input_dim, num_hidden_dim),\n",
    "                                      allow_deferred_init=True)\n",
    "            self.w2 = self.params.get('w2', shape=(num_hidden_dim, num_output_dim),\n",
    "                                      allow_deferred_init=True)\n",
    "\n",
    "    def hybrid_forward(self, F, x, w1, w2):\n",
    "        h = x.dot(w1)  # equivalent to F.np.dot(x, w1)\n",
    "        h_relu = F.np.ext.relu(h)  # equivalent to F.relu(h)\n",
    "        y_pred = h_relu.dot(w2)  # equivalent to F.np.dot(h_relu, w2)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class TotalLoss(gluon.HybridBlock):\n",
    "    def hybrid_forward(self, F, pred, label):\n",
    "        return ((pred - label) ** 2).sum()  # equivalent to F.np.sum(F.np.square(pred - label))\n",
    "\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.initialize(mx.init.Normal())\n",
    "regressor.hybridize()\n",
    "\n",
    "# Create random input and output data\n",
    "x = mx.nd.random.normal(shape=(64, 1000)).as_np_ndarray()  # x is of type mxnet.numpy.ndarray\n",
    "y = mx.nd.random.normal(shape=(64, 10)).as_np_ndarray()  # y is of type mxnet.numpy.ndarray\n",
    "\n",
    "total_loss = TotalLoss()\n",
    "trainer = gluon.Trainer(regressor.collect_params(), 'sgd', {'learning_rate': 1e-3, 'momentum': 0.9})\n",
    "\n",
    "for t in range(1000):\n",
    "    with autograd.record():\n",
    "        output = regressor(x)  # output is a type of np.ndarray because np.dot is the last op in the network\n",
    "        loss = total_loss(output, y)  # loss is a scalar np.ndarray\n",
    "    loss.backward()\n",
    "    print(t, loss)  # note that loss.asnumpy() is called\n",
    "    trainer.step(1)\n",
    "    if sw is not None:\n",
    "        sw.add_scalar('loss', loss.item(), global_step=t)  # loss.item() copies the tensor element to a python scalar\n",
    "        if t % 50 == 0:\n",
    "            for k, v in regressor.collect_params().items():\n",
    "                sw.add_histogram(tag=k, values=v.data(), global_step=t)\n",
    "\n",
    "if sw is not None:\n",
    "    sw.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
